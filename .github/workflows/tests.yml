name: Integration Tests

on:
  push:
    branches: [ main, dev ]
  pull_request:
    branches: [ main, dev ]
  workflow_dispatch:

jobs:
  tests:
    name: Backend Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Install yq
        run: |
          sudo wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/local/bin/yq
          sudo chmod +x /usr/local/bin/yq

      - name: Setup Kubernetes (k3s) and Kubeconfig
        run: |
          curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--disable=traefik --tls-san host.docker.internal" sh -
          mkdir -p $HOME/.kube
          sudo k3s kubectl config view --raw > $HOME/.kube/config
          sudo chmod 600 $HOME/.kube/config
          timeout 90 bash -c 'until sudo k3s kubectl cluster-info; do sleep 5; done'
          kubectl version
          kubectl get nodes

      - name: Modify Docker Compose for CI
        run: |
          cp docker-compose.yaml docker-compose.ci.yaml
          # Drop the frontend service for backend-only tests
          yq eval 'del(.services.frontend)' -i docker-compose.ci.yaml
          # For the backend service (extra_hosts already exists, skip it)
          # Note: backend.environment is a list in docker-compose.yaml
          yq eval '.services.backend.environment += ["TESTING=true"]' -i docker-compose.ci.yaml
          yq eval '.services.backend.environment += ["MONGO_ROOT_USER=root"]' -i docker-compose.ci.yaml
          yq eval '.services.backend.environment += ["MONGO_ROOT_PASSWORD=rootpassword"]' -i docker-compose.ci.yaml
          # Disable OpenTelemetry SDK during tests to avoid exporter retries
          yq eval '.services.backend.environment += ["OTEL_SDK_DISABLED=true"]' -i docker-compose.ci.yaml

          # MongoDB service already has defaults in docker-compose.yaml (root/rootpassword)
          # No need to override them

          # Disable SASL authentication for Kafka and Zookeeper in CI
          yq eval 'del(.services.kafka.environment.KAFKA_OPTS)' -i docker-compose.ci.yaml
          yq eval 'del(.services.zookeeper.environment.KAFKA_OPTS)' -i docker-compose.ci.yaml
          yq eval 'del(.services.zookeeper.environment.ZOOKEEPER_AUTH_PROVIDER_1)' -i docker-compose.ci.yaml
          yq eval '.services.kafka.volumes = [.services.kafka.volumes[] | select(. | contains("jaas.conf") | not)]' -i docker-compose.ci.yaml
          yq eval '.services.zookeeper.volumes = [.services.zookeeper.volumes[] | select(. | contains("/etc/kafka") | not)]' -i docker-compose.ci.yaml

          # Simplify Zookeeper for CI
          yq eval '.services.zookeeper.environment.ZOOKEEPER_4LW_COMMANDS_WHITELIST = "ruok,srvr"' -i docker-compose.ci.yaml
          # Disable zookeeper healthcheck in CI (use service_started instead)
          yq eval 'del(.services.zookeeper.healthcheck)' -i docker-compose.ci.yaml
          # Make Kafka start as soon as Zookeeper starts (not healthy)
          yq eval '.services.kafka.depends_on.zookeeper.condition = "service_started"' -i docker-compose.ci.yaml

          # For the cert-generator service
          # Check if extra_hosts exists, if not create it as a list
          yq eval 'select(.services."cert-generator".extra_hosts == null).services."cert-generator".extra_hosts = []' -i docker-compose.ci.yaml
          yq eval '.services."cert-generator".extra_hosts += ["host.docker.internal:host-gateway"]' -i docker-compose.ci.yaml
          yq eval '.services."cert-generator".environment += ["CI=true"]' -i docker-compose.ci.yaml
          yq eval '.services."cert-generator".volumes += [env(HOME) + "/.kube/config:/root/.kube/config:ro"]' -i docker-compose.ci.yaml

          echo "--- Modified docker-compose.ci.yaml ---"
          cat docker-compose.ci.yaml
          echo "------------------------------------"

      - name: Start services and check status
        run: |
            echo "Attempting to start services..."
            # Try to start services. If the command fails...
            docker compose -f docker-compose.ci.yaml up --build -d --remove-orphans || \
            (
              # ...then execute this block of code.
              echo "::error::Docker Compose failed to start. Dumping all logs..."
              docker compose -f docker-compose.ci.yaml logs
              exit 1 # Ensure the job fails
            )
            
            echo "Services started. Waiting for stabilization..."
            sleep 45
            
            echo "Final status of all containers:"
            docker compose -f docker-compose.ci.yaml ps
            
            # Explicitly check for containers that have exited
            if docker compose -f docker-compose.ci.yaml ps | grep -q 'Exit'; then
              echo "::error::One or more containers have exited unexpectedly. See logs above."
              docker compose -f docker-compose.ci.yaml logs --no-color
              exit 1
            fi

      - name: Wait for backend to be healthy
        run: |
          timeout 300 bash -c 'until curl -k https://127.0.0.1:443/api/v1/health -o /dev/null; do \
            echo "Retrying backend health check..."; \
            sleep 5; \
          done'
          echo "Backend is healthy!"

      # Frontend is excluded in backend-only CI; skip UI readiness

      - name: Check K8s setup status after startup
        run: |
          kubectl get pods -A -o wide
          kubectl get services -A -o wide
          kubectl get sa -n default
          kubectl get roles -n default
          kubectl get rolebindings -n default

      - name: Set up Python for Tests
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install Python test dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y python3 python3-pip
          cd backend
          pip3 install -r requirements.txt
          pip3 install -r requirements-dev.txt

      - name: Run backend tests with coverage
        env:
          BACKEND_BASE_URL: https://127.0.0.1:443
          # Use default MongoDB credentials for CI
          MONGO_ROOT_USER: root
          MONGO_ROOT_PASSWORD: rootpassword
          MONGODB_HOST: 127.0.0.1
          MONGODB_PORT: 27017
          # Explicit URL with default credentials
          MONGODB_URL: mongodb://root:rootpassword@127.0.0.1:27017/?authSource=admin
        run: |
          cd backend
          echo "Using BACKEND_BASE_URL=$BACKEND_BASE_URL"
          echo "MongoDB connection will use default CI credentials"
          python -m pytest tests/integration tests/unit -v --cov=app --cov-branch --cov-report=xml --cov-report=term --cov-report=term-missing

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          file: backend/coverage.xml
          flags: backend
          name: backend-coverage
          slug: HardMax71/Integr8sCode
          fail_ci_if_error: false

      - name: Collect logs
        if: always()
        run: |
          mkdir -p logs
          docker compose -f docker-compose.ci.yaml logs > logs/docker-compose.log
          docker compose -f docker-compose.ci.yaml logs cert-generator > logs/cert-generator.log
          docker compose -f docker-compose.ci.yaml logs backend > logs/backend.log
          docker compose -f docker-compose.ci.yaml logs mongo > logs/mongo.log
          kubectl get events --sort-by='.metadata.creationTimestamp' > logs/k8s-events.log
          kubectl get pods -A -o wide > logs/k8s-pods-final.log
          kubectl describe pods -A > logs/k8s-describe-pods-final.log

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-logs
          path: logs/
