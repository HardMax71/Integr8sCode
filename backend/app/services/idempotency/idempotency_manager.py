import asyncio
import hashlib
import json
from datetime import datetime, timedelta, timezone
from typing import Protocol

from pydantic import BaseModel
from pymongo.errors import DuplicateKeyError

from app.core.logging import logger
from app.core.metrics.context import get_database_metrics
from app.domain.idempotency import IdempotencyRecord, IdempotencyStats, IdempotencyStatus
from app.infrastructure.kafka.events import BaseEvent


class IdempotencyResult(BaseModel):
    is_duplicate: bool
    status: IdempotencyStatus
    created_at: datetime
    completed_at: datetime | None = None
    processing_duration_ms: int | None = None
    error: str | None = None
    has_cached_result: bool = False
    key: str


class IdempotencyConfig(BaseModel):
    key_prefix: str = "idempotency"
    default_ttl_seconds: int = 3600
    processing_timeout_seconds: int = 300
    enable_result_caching: bool = True
    max_result_size_bytes: int = 1048576
    enable_metrics: bool = True
    collection_name: str = "idempotency_keys"


class IdempotencyKeyStrategy:
    @staticmethod
    def event_based(event: BaseEvent) -> str:
        return f"{event.event_type}:{event.event_id}"

    @staticmethod
    def content_hash(event: BaseEvent, fields: set[str] | None = None) -> str:
        event_dict = event.model_dump()
        event_dict.pop("event_id", None)
        event_dict.pop("timestamp", None)
        event_dict.pop("metadata", None)

        if fields:
            event_dict = {k: v for k, v in event_dict.items() if k in fields}

        content = json.dumps(event_dict, sort_keys=True)
        return hashlib.sha256(content.encode()).hexdigest()

    @staticmethod
    def custom(event: BaseEvent, custom_key: str) -> str:
        return f"{event.event_type}:{custom_key}"


class IdempotencyRepoProtocol(Protocol):
    async def find_by_key(self, key: str) -> IdempotencyRecord | None: ...
    async def insert_processing(self, record: IdempotencyRecord) -> None: ...
    async def update_record(self, record: IdempotencyRecord) -> int: ...
    async def delete_key(self, key: str) -> int: ...
    async def aggregate_status_counts(self, key_prefix: str) -> dict[str, int]: ...
    async def health_check(self) -> None: ...


class IdempotencyManager:
    def __init__(self, config: IdempotencyConfig, repository: IdempotencyRepoProtocol) -> None:
        self.config = config
        self.metrics = get_database_metrics()
        self._repo: IdempotencyRepoProtocol = repository
        self._stats_update_task: asyncio.Task[None] | None = None

    async def initialize(self) -> None:
        if self.config.enable_metrics and self._stats_update_task is None:
            self._stats_update_task = asyncio.create_task(self._update_stats_loop())
        logger.info("Idempotency manager ready")

    async def close(self) -> None:
        if self._stats_update_task:
            self._stats_update_task.cancel()
            try:
                await self._stats_update_task
            except asyncio.CancelledError:
                pass
        logger.info("Closed idempotency manager")

    def _generate_key(
            self,
            event: BaseEvent,
            key_strategy: str,
            custom_key: str | None = None,
            fields: set[str] | None = None
    ) -> str:
        if key_strategy == "event_based":
            key = IdempotencyKeyStrategy.event_based(event)
        elif key_strategy == "content_hash":
            key = IdempotencyKeyStrategy.content_hash(event, fields)
        elif key_strategy == "custom" and custom_key:
            key = IdempotencyKeyStrategy.custom(event, custom_key)
        else:
            raise ValueError(f"Invalid key strategy: {key_strategy}")
        return f"{self.config.key_prefix}:{key}"

    

    async def check_and_reserve(
            self,
            event: BaseEvent,
            key_strategy: str = "event_based",
            custom_key: str | None = None,
            ttl_seconds: int | None = None,
            fields: set[str] | None = None,
    ) -> IdempotencyResult:
        full_key = self._generate_key(event, key_strategy, custom_key, fields)
        ttl = ttl_seconds or self.config.default_ttl_seconds

        existing = await self._repo.find_by_key(full_key)
        if existing:
            self.metrics.record_idempotency_cache_hit(event.event_type, "check_and_reserve")
            return await self._handle_existing_key(existing, full_key, event.event_type)

        self.metrics.record_idempotency_cache_miss(event.event_type, "check_and_reserve")
        return await self._create_new_key(full_key, event, ttl)

    async def _handle_existing_key(
            self,
            existing: IdempotencyRecord,
            full_key: str,
            event_type: str,
    ) -> IdempotencyResult:
        status = existing.status
        if status == IdempotencyStatus.PROCESSING:
            return await self._handle_processing_key(existing, full_key, event_type)

        self.metrics.record_idempotency_duplicate_blocked(event_type)
        created_at = existing.created_at or datetime.now(timezone.utc)
        return IdempotencyResult(
            is_duplicate=True,
            status=status,
            created_at=created_at,
            completed_at=existing.completed_at,
            processing_duration_ms=existing.processing_duration_ms,
            error=existing.error,
            has_cached_result=existing.result_json is not None,
            key=full_key,
        )

    async def _handle_processing_key(
            self,
            existing: IdempotencyRecord,
            full_key: str,
            event_type: str,
    ) -> IdempotencyResult:
        created_at = existing.created_at
        now = datetime.now(timezone.utc)

        if now - created_at > timedelta(seconds=self.config.processing_timeout_seconds):
            logger.warning(f"Idempotency key {full_key} processing timeout, allowing retry")
            existing.created_at = now
            existing.status = IdempotencyStatus.PROCESSING
            await self._repo.update_record(existing)
            return IdempotencyResult(is_duplicate=False, status=IdempotencyStatus.PROCESSING, created_at=now,
                                     key=full_key)

        self.metrics.record_idempotency_duplicate_blocked(event_type)
        return IdempotencyResult(is_duplicate=True, status=IdempotencyStatus.PROCESSING, created_at=created_at,
                                 has_cached_result=existing.result_json is not None, key=full_key)

    async def _create_new_key(self, full_key: str, event: BaseEvent, ttl: int) -> IdempotencyResult:
        created_at = datetime.now(timezone.utc)
        try:
            record = IdempotencyRecord(
                key=full_key,
                status=IdempotencyStatus.PROCESSING,
                event_type=event.event_type,
                event_id=str(event.event_id),
                created_at=created_at,
                ttl_seconds=ttl,
            )
            await self._repo.insert_processing(record)
            return IdempotencyResult(is_duplicate=False, status=IdempotencyStatus.PROCESSING, created_at=created_at,
                                     key=full_key)
        except DuplicateKeyError:
            # Race: someone inserted the same key concurrently â€” treat as existing
            existing = await self._repo.find_by_key(full_key)
            if existing:
                return await self._handle_existing_key(existing, full_key, event.event_type)
            # If for some reason it's still not found, allow processing
            return IdempotencyResult(is_duplicate=False, status=IdempotencyStatus.PROCESSING, created_at=created_at,
                                     key=full_key)

    async def _update_key_status(
            self,
            full_key: str,
            existing: IdempotencyRecord,
            status: IdempotencyStatus,
            cached_json: str | None = None,
            error: str | None = None,
    ) -> bool:
        created_at = existing.created_at
        completed_at = datetime.now(timezone.utc)
        duration_ms = int((completed_at - created_at).total_seconds() * 1000)
        existing.status = status
        existing.completed_at = completed_at
        existing.processing_duration_ms = duration_ms
        if error:
            existing.error = error
        if cached_json is not None and self.config.enable_result_caching:
            if len(cached_json.encode()) <= self.config.max_result_size_bytes:
                existing.result_json = cached_json
            else:
                logger.warning(f"Result too large to cache for key {full_key}")
        return (await self._repo.update_record(existing)) > 0

    async def mark_completed(
            self,
            event: BaseEvent,
            key_strategy: str = "event_based",
            custom_key: str | None = None,
            fields: set[str] | None = None
    ) -> bool:
        full_key = self._generate_key(event, key_strategy, custom_key, fields)
        try:
            existing = await self._repo.find_by_key(full_key)
        except Exception as e:  # Narrow DB op
            logger.error(f"Failed to load idempotency key for completion: {e}")
            return False
        if not existing:
            logger.warning(f"Idempotency key {full_key} not found when marking completed")
            return False
        # mark_completed does not accept arbitrary result today; use mark_completed_with_cache for cached payloads
        return await self._update_key_status(full_key, existing, IdempotencyStatus.COMPLETED, cached_json=None)

    async def mark_failed(
            self,
            event: BaseEvent,
            error: str,
            key_strategy: str = "event_based",
            custom_key: str | None = None,
            fields: set[str] | None = None
    ) -> bool:
        full_key = self._generate_key(event, key_strategy, custom_key, fields)
        existing = await self._repo.find_by_key(full_key)
        if not existing:
            logger.warning(f"Idempotency key {full_key} not found when marking failed")
            return False
        return await self._update_key_status(full_key, existing, IdempotencyStatus.FAILED, cached_json=None,
                                             error=error)

    async def mark_completed_with_json(
            self,
            event: BaseEvent,
            cached_json: str,
            key_strategy: str = "event_based",
            custom_key: str | None = None,
            fields: set[str] | None = None
    ) -> bool:
        full_key = self._generate_key(event, key_strategy, custom_key, fields)
        existing = await self._repo.find_by_key(full_key)
        if not existing:
            logger.warning(f"Idempotency key {full_key} not found when marking completed with cache")
            return False
        return await self._update_key_status(full_key, existing, IdempotencyStatus.COMPLETED, cached_json=cached_json)

    async def get_cached_json(self, event: BaseEvent, key_strategy: str, custom_key: str | None,
                              fields: set[str] | None = None) -> str:
        full_key = self._generate_key(event, key_strategy, custom_key, fields)
        existing = await self._repo.find_by_key(full_key)
        assert existing and existing.result_json is not None, "Invariant: cached result must exist when requested"
        return existing.result_json

    async def remove(
            self,
            event: BaseEvent,
            key_strategy: str = "event_based",
            custom_key: str | None = None,
            fields: set[str] | None = None
    ) -> bool:
        full_key = self._generate_key(event, key_strategy, custom_key, fields)
        try:
            deleted = await self._repo.delete_key(full_key)
            return deleted > 0
        except Exception as e:
            logger.error(f"Failed to remove idempotency key: {e}")
            return False

    async def get_stats(self) -> IdempotencyStats:
        counts_raw = await self._repo.aggregate_status_counts(self.config.key_prefix)
        status_counts: dict[IdempotencyStatus, int] = {
            IdempotencyStatus.PROCESSING: counts_raw.get(IdempotencyStatus.PROCESSING, 0),
            IdempotencyStatus.COMPLETED: counts_raw.get(IdempotencyStatus.COMPLETED, 0),
            IdempotencyStatus.FAILED: counts_raw.get(IdempotencyStatus.FAILED, 0),
        }
        total = sum(status_counts.values())
        return IdempotencyStats(total_keys=total, status_counts=status_counts, prefix=self.config.key_prefix)

    async def _update_stats_loop(self) -> None:
        while True:
            try:
                stats = await self.get_stats()
                self.metrics.update_idempotency_keys_active(stats.total_keys, self.config.key_prefix)
                await asyncio.sleep(60)
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Failed to update idempotency stats: {e}")
                await asyncio.sleep(300)


def create_idempotency_manager(
        *,
        repository: IdempotencyRepoProtocol,
        config: IdempotencyConfig | None = None,
) -> IdempotencyManager:
    return IdempotencyManager(config or IdempotencyConfig(), repository)
